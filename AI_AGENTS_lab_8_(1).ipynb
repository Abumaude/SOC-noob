{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abumaude/SOC-noob/blob/main/AI_AGENTS_lab_8_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents and their Types\n",
        "\n",
        "**An agent** is a system or program that can autonomously perform tasks on behalf of a user or another system. These agents interact with their environment, collect data, and use this data to make decisions and take actions to achieve specific goals.\n",
        "\n",
        "**AI agents can vary in complexity and functionality. Here are some key characteristics:**\n",
        "\n",
        "\n",
        "- **Autonomy**: They operate without human intervention, making decisions based on their programming and the data they collect.\n",
        "\n",
        "- **Perception**: They use sensors or data inputs to perceive their environment.\n",
        "\n",
        "- **Action**: They can take actions to influence their environment, such as moving, speaking, or making decisions.\n",
        "\n",
        "- **Rationality**: They aim to achieve their goals in the most efficient way possible, often using algorithms to determine the best course of action.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eDn8biBDuwtq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI agents** can be categorized into several types based on their capabilities and how they interact with their environment. Here are the main types:\n",
        "\n"
      ],
      "metadata": {
        "id": "dxUqd0Aivu1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Simple Reflex Agents**: These agents act only based on the current percept, ignoring the rest of the percept history. They follow a set of predefined rules to respond to specific situations. For example, a thermostat that turns on the heater when the temperature drops below a certain point.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YCimuyMdEJCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple reflex agent for a thermostat\n",
        "class ThermostatAgent:\n",
        "    def __init__(self, temperature_threshold):\n",
        "        self.temperature_threshold = temperature_threshold\n",
        "        self.heater_on = False\n",
        "\n",
        "    def perceive(self, current_temperature):\n",
        "        self.current_temperature = current_temperature\n",
        "\n",
        "    def act(self):\n",
        "        if self.current_temperature < self.temperature_threshold:\n",
        "            self.heater_on = True\n",
        "            print(\"Heater turned ON\")\n",
        "        else:\n",
        "            self.heater_on = False\n",
        "            print(\"Heater turned OFF\")\n",
        "\n",
        "# Example usage\n",
        "thermostat = ThermostatAgent(20)  # Threshold temperature is 20 degrees\n",
        "\n",
        "# Simulate temperature readings\n",
        "temperatures = [18, 22, 19, 25, 15]\n",
        "\n",
        "for temp in temperatures:\n",
        "  thermostat.perceive(temp)\n",
        "  thermostat.act()"
      ],
      "metadata": {
        "id": "QdaVJBEBv_BK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735d2c1d-be49-4451-fa95-03723cbbcc9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heater turned ON\n",
            "Heater turned OFF\n",
            "Heater turned ON\n",
            "Heater turned OFF\n",
            "Heater turned ON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1: Simple Reflex Agent**:\n",
        "   - **Description**: Implement a simple reflex agent for a basic environment, such as a vacuum cleaner that cleans a room.\n",
        "   - **Requirements**: The agent should move around a grid and clean any dirty spots it encounters based on predefined rules."
      ],
      "metadata": {
        "id": "apyIxi8IMpjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Model-Based Reflex Agents:** These agents maintain an internal model of the world, which helps them handle more complex situations by considering the history of percepts. They can make decisions based on both current and past information."
      ],
      "metadata": {
        "id": "yfcGRXDUwNMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model-Based Reflex Agents Example (Expanding on the Thermostat)\n",
        "\n",
        "class ModelBasedThermostatAgent:\n",
        "    def __init__(self, temperature_threshold, learning_rate=0.1):\n",
        "        self.temperature_threshold = temperature_threshold\n",
        "        self.heater_on = False\n",
        "        self.internal_temperature_model = 20  # Initial temperature guess\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def perceive(self, current_temperature):\n",
        "        self.current_temperature = current_temperature\n",
        "\n",
        "    def update_model(self):\n",
        "        # Simple model update based on current temperature and error\n",
        "        error = self.current_temperature - self.internal_temperature_model\n",
        "        self.internal_temperature_model += error * self.learning_rate\n",
        "\n",
        "    def act(self):\n",
        "        self.update_model()  # Update the model first\n",
        "\n",
        "        if self.internal_temperature_model < self.temperature_threshold:\n",
        "            self.heater_on = True\n",
        "            print(f\"Heater turned ON (Model temp: {self.internal_temperature_model:.2f}, Actual temp: {self.current_temperature})\")\n",
        "        else:\n",
        "            self.heater_on = False\n",
        "            print(f\"Heater turned OFF (Model temp: {self.internal_temperature_model:.2f}, Actual temp: {self.current_temperature})\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "model_thermostat = ModelBasedThermostatAgent(20)\n",
        "\n",
        "# Simulate temperature readings\n",
        "temperatures = [18, 22, 19, 25, 15]\n",
        "\n",
        "for temp in temperatures:\n",
        "    model_thermostat.perceive(temp)\n",
        "    model_thermostat.act()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "g0cKA7PlxGhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e98853-e6d5-462f-9708-099a48387c59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Heater turned ON (Model temp: 19.80, Actual temp: 18)\n",
            "Heater turned OFF (Model temp: 20.02, Actual temp: 22)\n",
            "Heater turned ON (Model temp: 19.92, Actual temp: 19)\n",
            "Heater turned OFF (Model temp: 20.43, Actual temp: 25)\n",
            "Heater turned ON (Model temp: 19.88, Actual temp: 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2: Model-Based Reflex Agent**:\n",
        "   - **Description**: Enhance the vacuum cleaner agent to remember which spots it has already cleaned.\n",
        "   - **Requirements**: The agent should maintain an internal state to avoid re-cleaning the same spot.\n"
      ],
      "metadata": {
        "id": "hFe1NDKpMyWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3- Goal-Based Agents**: These agents act to achieve specific goals. They use their internal model to evaluate different actions and choose the one that brings them closer to their goal. For instance, a navigation system that plans a route to a destination."
      ],
      "metadata": {
        "id": "yWec25DAwnN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Goal-Based Agent Example (Navigation)\n",
        "\n",
        "class NavigationAgent:\n",
        "    def __init__(self, destination):\n",
        "        self.destination = destination\n",
        "        self.current_location = (0, 0)  # Initial location\n",
        "        self.map = {  # Simplified map representation\n",
        "            (0, 0): [(1, 0), (0, 1)],\n",
        "            (1, 0): [(0, 0), (2, 0)],\n",
        "            (0, 1): [(0, 0), (0, 2)],\n",
        "            (2, 0): [(1, 0)],\n",
        "            (0, 2): [(0,1), (1,2)],\n",
        "            (1,2): [(0,2), (2,2)],\n",
        "            (2,2): [(1,2)]\n",
        "        }\n",
        "\n",
        "    def perceive(self, current_location):\n",
        "        self.current_location = current_location\n",
        "\n",
        "    def plan_route(self):\n",
        "      # Simple route planning (replace with a better algorithm)\n",
        "      queue = [(self.current_location, [self.current_location])]\n",
        "      visited = set()\n",
        "      while queue:\n",
        "          current, path = queue.pop(0)\n",
        "          if current == self.destination:\n",
        "              return path\n",
        "          visited.add(current)\n",
        "          for neighbor in self.map.get(current, []):\n",
        "              if neighbor not in visited:\n",
        "                  queue.append((neighbor, path + [neighbor]))\n",
        "      return None\n",
        "\n",
        "\n",
        "    def act(self):\n",
        "        route = self.plan_route()\n",
        "        if route:\n",
        "            if len(route) > 1:\n",
        "                next_location = route[1]\n",
        "                print(f\"Moving from {self.current_location} to {next_location}\")\n",
        "                self.current_location = next_location # Update current location\n",
        "            else:\n",
        "                print(f\"Arrived at destination {self.destination}\")\n",
        "\n",
        "        else:\n",
        "            print(\"No route found to the destination.\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "navigator = NavigationAgent((2, 2))\n",
        "\n",
        "# Simulate the agent's journey\n",
        "locations = [(0,0), (1,0), (2,0), (1,0), (0,1), (0,2), (1,2), (2,2)]\n",
        "\n",
        "for location in locations:\n",
        "  navigator.perceive(location)\n",
        "  navigator.act()"
      ],
      "metadata": {
        "id": "tDmnZyXvxqwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10ec7329-cc91-4281-b0d6-b7d976499a6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving from (0, 0) to (0, 1)\n",
            "Moving from (1, 0) to (0, 0)\n",
            "Moving from (2, 0) to (1, 0)\n",
            "Moving from (1, 0) to (0, 0)\n",
            "Moving from (0, 1) to (0, 2)\n",
            "Moving from (0, 2) to (1, 2)\n",
            "Moving from (1, 2) to (2, 2)\n",
            "Arrived at destination (2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3: Goal-Based Agent**:\n",
        "   - **Description**: Implement a navigation agent that finds the shortest path to a goal in a maze.\n",
        "   - **Requirements**: The agent should use a search algorithm (e.g., A*) to reach the goal efficiently.\n"
      ],
      "metadata": {
        "id": "QfO1TtE-MynE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4- Utility-Based Agents**: These agents not only aim to achieve goals but also consider the best way to achieve them by evaluating the utility (or value) of different actions. They strive to maximize their performance measure.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MAiP9fPHyBBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility-Based Agent Example (Resource Allocation)\n",
        "\n",
        "class ResourceAllocationAgent:\n",
        "    def __init__(self, resources, tasks):\n",
        "        self.resources = resources  # Available resources (e.g., budget, time)\n",
        "        self.tasks = tasks  # List of tasks with utilities and resource requirements\n",
        "\n",
        "    def evaluate_utility(self, task, allocated_resources):\n",
        "        # A simple utility function (replace with a more complex one if needed)\n",
        "        if allocated_resources >= task[\"resource_requirements\"]:\n",
        "          return task[\"utility\"] * (allocated_resources / task[\"resource_requirements\"]) # Higher utility for more resources\n",
        "        else:\n",
        "          return 0 # Cannot perform the task\n",
        "\n",
        "    def allocate_resources(self):\n",
        "        remaining_resources = self.resources\n",
        "        allocation_plan = {}\n",
        "\n",
        "        sorted_tasks = sorted(self.tasks, key=lambda task: task[\"utility\"], reverse=True)\n",
        "\n",
        "        for task in sorted_tasks:\n",
        "            # Allocate resources if they are available\n",
        "            if remaining_resources >= task[\"resource_requirements\"]:\n",
        "                allocated_amount = task[\"resource_requirements\"]\n",
        "                allocation_plan[task[\"name\"]] = allocated_amount\n",
        "                remaining_resources -= allocated_amount\n",
        "            else:\n",
        "                allocation_plan[task[\"name\"]] = 0 # No resources for this task\n",
        "\n",
        "        return allocation_plan\n",
        "\n",
        "    def act(self):\n",
        "        allocation_plan = self.allocate_resources()\n",
        "\n",
        "        total_utility = 0\n",
        "        for task_name, allocated_resources in allocation_plan.items():\n",
        "          task = next((task for task in self.tasks if task[\"name\"] == task_name), None)\n",
        "          if task:\n",
        "            utility = self.evaluate_utility(task, allocated_resources)\n",
        "            total_utility += utility\n",
        "            print(f\"Task: {task_name}, Allocated Resources: {allocated_resources}, Utility: {utility}\")\n",
        "\n",
        "        print(f\"Total Utility Achieved: {total_utility}\")\n",
        "\n",
        "# Example usage\n",
        "tasks = [\n",
        "    {\"name\": \"Task A\", \"utility\": 10, \"resource_requirements\": 5},\n",
        "    {\"name\": \"Task B\", \"utility\": 5, \"resource_requirements\": 2},\n",
        "    {\"name\": \"Task C\", \"utility\": 8, \"resource_requirements\": 3},\n",
        "]\n",
        "\n",
        "agent = ResourceAllocationAgent(resources=10, tasks=tasks)\n",
        "agent.act()"
      ],
      "metadata": {
        "id": "whlj5tmqxA82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f218e6a-0715-417e-fbc6-ae7e300d18c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task: Task A, Allocated Resources: 5, Utility: 10.0\n",
            "Task: Task C, Allocated Resources: 3, Utility: 8.0\n",
            "Task: Task B, Allocated Resources: 2, Utility: 5.0\n",
            "Total Utility Achieved: 23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 4: Utility-Based Agent**:\n",
        "   - **Description**: Create an agent that not only reaches the goal but also maximizes a utility function, such as collecting items of value along the way.\n",
        "   - **Requirements**: The agent should evaluate different paths based on their utility and choose the most beneficial one.\n"
      ],
      "metadata": {
        "id": "AKQKi2YRM0d2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5- Learning Agents:** These agents have the ability to learn from their experiences and improve their performance over time. They can adapt to new situations by updating their knowledge base and decision-making processes. More will be introduced next labs.  "
      ],
      "metadata": {
        "id": "kc0SFmauw2DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning Agent Example (Simple Reinforcement Learning)\n",
        "\n",
        "import random\n",
        "\n",
        "class LearningAgent:\n",
        "    def __init__(self, actions):\n",
        "        self.actions = actions\n",
        "        self.q_table = {}  # Q-table to store Q-values\n",
        "        self.learning_rate = 0.1\n",
        "        self.discount_factor = 0.9\n",
        "        self.exploration_rate = 0.1\n",
        "\n",
        "    def get_q_value(self, state, action):\n",
        "        return self.q_table.get((state, action), 0)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            # Explore: Choose a random action\n",
        "            return random.choice(self.actions)\n",
        "        else:\n",
        "            # Exploit: Choose the action with the highest Q-value\n",
        "            q_values = [self.get_q_value(state, action) for action in self.actions]\n",
        "            return self.actions[q_values.index(max(q_values))]\n",
        "\n",
        "    def learn(self, state, action, reward, next_state):\n",
        "        # Q-learning update rule\n",
        "        old_q_value = self.get_q_value(state, action)\n",
        "        next_max_q = max([self.get_q_value(next_state, a) for a in self.actions])\n",
        "        new_q_value = (1 - self.learning_rate) * old_q_value + self.learning_rate * (reward + self.discount_factor * next_max_q)\n",
        "        self.q_table[(state, action)] = new_q_value\n",
        "\n",
        "# Example usage (simplified environment)\n",
        "\n",
        "actions = [\"left\", \"right\"]  # Possible actions\n",
        "agent = LearningAgent(actions)\n",
        "environment_states = {\n",
        "    \"A\": {\"left\": (\"B\", -1), \"right\": (\"C\", 1)},\n",
        "    \"B\": {\"left\": (\"A\", -1), \"right\": (\"D\", 10)},\n",
        "    \"C\": {\"left\": (\"A\", -1), \"right\": (\"E\", -5)},\n",
        "    \"D\": {\"left\": (\"B\", -1), \"right\": (\"D\", 10)}, # Example of terminal state with high reward\n",
        "    \"E\": {\"left\": (\"C\", -1), \"right\": (\"E\", -5)}, # Example of terminal state with negative reward\n",
        "\n",
        "}\n",
        "current_state = \"A\"\n",
        "\n",
        "for episode in range(100): # Run for 100 episodes\n",
        "  current_state = \"A\"  # Reset to initial state at start of each episode\n",
        "  for _ in range(10): # Limit episode steps\n",
        "      action = agent.choose_action(current_state)\n",
        "      next_state, reward = environment_states[current_state][action]\n",
        "      agent.learn(current_state, action, reward, next_state)\n",
        "      current_state = next_state\n",
        "\n",
        "# Print learned Q-values\n",
        "print(\"Learned Q-values:\")\n",
        "for (state, action), q_value in agent.q_table.items():\n",
        "    print(f\"State: {state}, Action: {action}, Q-value: {q_value}\")"
      ],
      "metadata": {
        "id": "6KqS9NWUyO0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d28ce4b6-fb27-45a7-9a6d-16383e12274f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learned Q-values:\n",
            "State: A, Action: left, Q-value: 88.27688049680964\n",
            "State: B, Action: right, Q-value: 99.69883399090406\n",
            "State: D, Action: left, Q-value: 83.05368017675363\n",
            "State: D, Action: right, Q-value: 99.85739101869719\n",
            "State: A, Action: right, Q-value: 2.189914374527203\n",
            "State: C, Action: left, Q-value: 18.77998713902596\n",
            "State: C, Action: right, Q-value: -1.355\n",
            "State: E, Action: left, Q-value: -0.2423657715306539\n",
            "State: E, Action: right, Q-value: -0.5\n",
            "State: B, Action: left, Q-value: 28.0412587556315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 5: Learning Agents:**\n",
        "\n",
        "Try to understand the basic steps in this code, then write down your step-by-step explanation.\n",
        "\n",
        "Reinforcement Learning* algorithms will be the topic of next week"
      ],
      "metadata": {
        "id": "NeeuvqfWKnJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 6: Future is Agentic**\n",
        "\n",
        "Listen to the video uploaded to Canvas by one of AI pioneers (Andrew NG) about a future powered by AI agents... After that please answer the following questions:\n",
        "\n",
        "\n",
        "What is an agentic *workflow*, and how does it differ from a non-agentic workflow?\n",
        "\n",
        "Can you provide real-world examples of agentic workflows beyond those mentioned in the video?\n",
        "\n",
        "How can agentic workflows be applied to various industries, such as healthcare, finance, or education?\n",
        "\n",
        "As AI agents become more autonomous, what ethical considerations should be taken into account?\n",
        "\n",
        "How can we ensure that AI agents are used responsibly and ethically?\n",
        "\n",
        "What are the potential societal implications of widespread adoption of AI agents?\n"
      ],
      "metadata": {
        "id": "fiIm6nnFDQSF"
      }
    }
  ]
}